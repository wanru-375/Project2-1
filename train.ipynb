{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#由于地图中左转较多，为防止小车易朝左偏出车道，对部分训练图片进行水平翻转，角度取负值。\n",
    "def horizontal_flip(img, degree):\n",
    "    choice = np.random.choice([0, 1])\n",
    "    if choice == 1:\n",
    "        img, degree = cv2.flip(img, 1), -degree\n",
    "    return (img, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行色彩空间转换并随机调整亮度。\n",
    "def random_brightness(img, degree):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    alpha = np.random.uniform(low = 0.1, high = 1.0, size = None)\n",
    "    v = hsv[:, :, 2]\n",
    "    v = v * alpha\n",
    "    hsv[:, :, 2] = v.astype('uint8')\n",
    "    rgb = cv2.cvtColor(hsv.astype('uint8'), cv2.COLOR_HSV2RGB)\n",
    "    return(rgb, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练数据中转向角为0的情况过多，将部分转向角为0的情况删除，设置丢弃率rate。\n",
    "def discard_zero_steering(degrees, rate):\n",
    "    steering_zero_idx = np.where(degrees == 0)\n",
    "    steering_zero_idx = steering_zero_idx[0]\n",
    "    size_del = int(len(steering_zero_idx) * rate)\n",
    "    return np.random.choice(steering_zero_idx, size = size_del, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools==3.3\n",
      "  Using cached setuptools-3.3-py2.py3-none-any.whl (545 kB)\n",
      "Installing collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 51.1.2\n",
      "    Uninstalling setuptools-51.1.2:\n",
      "      Successfully uninstalled setuptools-51.1.2\n",
      "Successfully installed setuptools-3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.4.0 requires setuptools>=41.0.0, but you have setuptools 3.3 which is incompatible.\n",
      "ipython 7.16.1 requires setuptools>=18.5, but you have setuptools 3.3 which is incompatible.\n",
      "google-auth 1.24.0 requires setuptools>=40.3.0, but you have setuptools 3.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#!pip install setuptools==3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#为实时查看迭代过程中loss变化，安装livelossplot\n",
    "#import sys \n",
    "#sys.version\n",
    "#! pip install livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网络训练（train.py）：\n",
    "import numpy as np\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, PReLU, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "import os.path\n",
    "import csv\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from keras import callbacks\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "SEED = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_flip(img, degree):\n",
    "    choice = np.random.choice([0, 1])\n",
    "    if choice == 1:\n",
    "        img, degree = cv2.flip(img, 1), -degree\n",
    "    return (img, degree)\n",
    "\n",
    "\n",
    "def random_brightness(img, degree):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    alpha = np.random.uniform(low=0.1, high=1.0, size=None)\n",
    "    v = hsv[:, :, 2]\n",
    "    v = v * alpha\n",
    "    hsv[:, :, 2] = v.astype('uint8')\n",
    "    rgb = cv2.cvtColor(hsv.astype('uint8'), cv2.COLOR_HSV2RGB)\n",
    "    return (rgb, degree)\n",
    "\n",
    "\n",
    "def left_right_random_swap(img_address, degree, degree_corr=1.0 / 4):\n",
    "    swap = np.random.choice(['L', 'R', 'C'])\n",
    "    if swap == 'L':\n",
    "        img_address = img_address.replace('center', 'left')\n",
    "        corrected_label = np.arctan(math.tan(degree) + degree_corr)\n",
    "        return (img_address, corrected_label)\n",
    "    elif swap == 'R':\n",
    "        img_address = img_address.replace('center', 'right')\n",
    "        corrected_label = np.arctan(math.tan(degree) - degree_corr)\n",
    "        return (img_address, corrected_label)\n",
    "    else:\n",
    "        return (img_address, degree)\n",
    "\n",
    "\n",
    "def discard_zero_steering(degrees, rate):\n",
    "    steering_zero_idx = np.where(degrees == 0)\n",
    "    steering_zero_idx = steering_zero_idx[0]\n",
    "    size_del = int(len(steering_zero_idx) * rate)\n",
    "    return np.random.choice(steering_zero_idx, size=size_del, replace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(shape):\n",
    "    '''\n",
    "    预测方向盘角度: 以图像为输入, 预测方向盘的转动角度\n",
    "    shape: 输入图像的尺寸, 例如(128, 128, 3)\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(8, (5, 5), strides=(1, 1), padding=\"valid\", input_shape=shape))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(8, (5, 5), strides=(1, 1), padding=\"valid\", ))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(16, (4, 4), strides=(1, 1), padding=\"valid\", ))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(16, (5, 5), strides=(1, 1), padding=\"valid\", ))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(50))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_transformation(img_address, degree, data_dir):\n",
    "    img_address, degree = left_right_random_swap(img_address, degree)\n",
    "    img = cv2.imread(img_address)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img, degree = random_brightness(img, degree)\n",
    "    img, degree = horizontal_flip(img, degree)\n",
    "    return (img, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x, y, batch_size, shape, training=True, data_dir='data/', monitor=True, yieldXY=True,\n",
    "                    discard_rate=0.95):\n",
    "    if training:\n",
    "        y_bag = []\n",
    "        x, y = shuffle(x, y)\n",
    "        new_x = x\n",
    "        new_y = y\n",
    "    else:\n",
    "        new_x = x\n",
    "        new_y = y\n",
    "\n",
    "    offset = 0\n",
    "    while True:\n",
    "        X = np.empty((batch_size, *shape))\n",
    "        Y = np.empty((batch_size, 1))\n",
    "\n",
    "        for example in range(batch_size):\n",
    "            img_address, img_steering = new_x[example + offset], new_y[example + offset]\n",
    "            print(img_address)\n",
    "            if training:\n",
    "                img, img_steering = image_transformation(img_address, img_steering, data_dir)\n",
    "            else:\n",
    "                # img = cv2.imread(data_dir + img_address)\n",
    "                img = cv2.imread(img_address)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            X[example, :, :, :] = cv2.resize(img[80:140, 0:320], (shape[0], shape[1])) / 255 - 0.5\n",
    "\n",
    "            Y[example] = img_steering\n",
    "            if training:\n",
    "                y_bag.append(img_steering)\n",
    "\n",
    "            if (example + 1) + offset > len(new_y) - 1:\n",
    "                x, y = shuffle(x, y)\n",
    "                rand_zero_idx = discard_zero_steering(y, rate=discard_rate)\n",
    "                new_x = x\n",
    "                new_y = y\n",
    "                new_x = np.delete(new_x, rand_zero_idx, axis=0)\n",
    "                new_y = np.delete(new_y, rand_zero_idx, axis=0)\n",
    "                offset = 0\n",
    "        if yieldXY:\n",
    "            yield (X, Y)\n",
    "        else:\n",
    "            yield X\n",
    "\n",
    "        offset = offset + batch_size\n",
    "        if training:\n",
    "            np.save('y_bag.npy', np.array(y_bag))\n",
    "            np.save('Xbatch_sample.npy', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8215\n",
      "24645\n",
      "batch size: 8\n",
      "Train set size: 6572 | Validation set size: 1643\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    data_path = 'D:\\\\.zhangwanru\\\\yanyi\\\\yijike\\\\jiqi\\\\CNN\\\\data\\\\'\n",
    "    with open(data_path+'driving_log.csv', 'r',encoding=\"utf-8\") as csvfile:\n",
    "        file_reader = csv.reader(csvfile, delimiter=',')\n",
    "        log = []\n",
    "        for row in file_reader:\n",
    "            log.append(row)\n",
    "\n",
    "    log = np.array(log)\n",
    "    print(len(log))\n",
    "    #print(len(ls_imgs))\n",
    "\n",
    "    # 判断图像文件数量是否等于csv日志文件中记录的数量\n",
    "    ls_imgs = glob.glob(data_path+'IMG\\\\*.jpg')\n",
    "    print(len(ls_imgs))\n",
    "    #glob.glob(r\"C:/Users/Zhangzhang/Desktop/IMG/*.jpg\"))\n",
    "    assert len(ls_imgs) == len(log) * 3, 'number of images does not match'\n",
    "    # 使用20%的数据作为测试数据\n",
    "    validation_ratio = 0.2\n",
    "    shape = (128, 128, 3)\n",
    "    batch_size = 8\n",
    "    nb_epoch = 400\n",
    "\n",
    "    x_ = log[:, 0]\n",
    "    y_ = log[:, 3].astype(float)\n",
    "    x_, y_ = shuffle(x_, y_)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(x_, y_, test_size=validation_ratio, random_state=SEED)\n",
    "\n",
    "    print('batch size: {}'.format(batch_size))\n",
    "    print('Train set size: {} | Validation set size: {}'.format(len(X_train), len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\.♥张婉如♥\\研一课程\\已结课\\机器学习与交通运输\\CNN\\data\\IMG\\center_2021_01_09_11_11_04_251.jpg\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-6uw63ony\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-8cd418749b0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                               callbacks=callbacks_list) \n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./trainHistoryDict.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1064\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    777\u001b[0m     \u001b[1;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m     \u001b[1;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m     \u001b[0mpeek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    834\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    837\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-d45db4f85a5b>\u001b[0m in \u001b[0;36mbatch_generator\u001b[1;34m(x, y, batch_size, shape, training, data_dir, monitor, yieldXY, discard_rate)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                 \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_steering\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_transformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_address\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_steering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[1;31m# img = cv2.imread(data_dir + img_address)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-71ed9dc9793b>\u001b[0m in \u001b[0;36mimage_transformation\u001b[1;34m(img_address, degree, data_dir)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mimg_address\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleft_right_random_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_address\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_brightness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhorizontal_flip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-6uw63ony\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "    samples_per_epoch = batch_size\n",
    "    # 使得validation数据量大小为batch_size的整数陪\n",
    "    nb_val_samples = len(y_val) - len(y_val) % batch_size\n",
    "    model = get_model(shape)\n",
    "    #print(model.summary())\n",
    "\n",
    "    # 根据validation loss保存最优模型\n",
    "    save_best = callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', verbose=1,save_best_only=True, mode='min')\n",
    "\n",
    "    # 如果训练持续没有validation loss的提升, 提前结束训练，可通过更改patience参数确定终止条件。\n",
    "    early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=400,verbose=0, mode='auto')\n",
    "    callbacks_list = [early_stop, save_best, PlotLossesKeras()]\n",
    "    #callbacks_list\n",
    "\n",
    "    #history = model.fit_generator(batch_generator(X_train, y_train, batch_size, shape, training=True),\n",
    "                                  #steps_per_epoch=samples_per_epoch,\n",
    "                                  #validation_steps=nb_val_samples//batch_size,\n",
    "                                  #validation_data=batch_generator(X_val, y_val, batch_size, shape,training=False, monitor=False),\n",
    "                                  #epochs=nb_epoch, \n",
    "                                  #verbose=1, \n",
    "                                  #callbacks=callbacks_list)\n",
    "    history = model.fit_generator(batch_generator(X_train, y_train, batch_size, shape, training=True),\n",
    "                                  steps_per_epoch = samples_per_epoch,\n",
    "                                  validation_steps = nb_val_samples // batch_size,\n",
    "                                  validation_data = batch_generator(X_val, y_val, batch_size, shape, training=False, monitor=False),\n",
    "                                  epochs=nb_epoch, \n",
    "                                  verbose=1, \n",
    "                                  callbacks=callbacks_list) \n",
    "\n",
    "    with open('./trainHistoryDict.p', 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    pyplot.plot(history.history['loss'])\n",
    "    pyplot.plot(history.history['val_loss'])\n",
    "    pyplot.title('model train vs validation loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'validation'], loc='upper right')\n",
    "    pyplot.savefig('train_val_loss.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, batch_size, shape, training=True),\n",
    "                                  steps_per_epoch=samples_per_epoch,\n",
    "                                  validation_steps=nb_val_samples//batch_size,\n",
    "                                  validation_data=batch_generator(X_val, y_val, batch_size, shape,training=False, monitor=False),\n",
    "                                  epochs=nb_epoch, \n",
    "                                  verbose=1, \n",
    "                                  callbacks=callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    " # 保存模型\n",
    "    with open('model.json', 'w') as f:\n",
    "        f.write(model.to_json())\n",
    "    model.save('model.h5')\n",
    "    print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.19.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 8\n",
      "Train set size: 6572 | Validation set size: 1643\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_116 (Conv2D)          (None, 124, 124, 8)       608       \n",
      "_________________________________________________________________\n",
      "activation_203 (Activation)  (None, 124, 124, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_116 (MaxPoolin (None, 62, 62, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_203 (Dropout)        (None, 62, 62, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_117 (Conv2D)          (None, 58, 58, 8)         1608      \n",
      "_________________________________________________________________\n",
      "activation_204 (Activation)  (None, 58, 58, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_117 (MaxPoolin (None, 29, 29, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_204 (Dropout)        (None, 29, 29, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_118 (Conv2D)          (None, 26, 26, 16)        2064      \n",
      "_________________________________________________________________\n",
      "activation_205 (Activation)  (None, 26, 26, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_118 (MaxPoolin (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_205 (Dropout)        (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_119 (Conv2D)          (None, 9, 9, 16)          6416      \n",
      "_________________________________________________________________\n",
      "activation_206 (Activation)  (None, 9, 9, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_119 (MaxPoolin (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_206 (Dropout)        (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_207 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_207 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "activation_208 (Activation)  (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_208 (Dropout)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_209 (Activation)  (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_209 (Dropout)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 50,563\n",
      "Trainable params: 50,563\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-6uw63ony\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-e73c23d6942d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                   validation_data = batch_generator(X_val, y_val, batch_size, shape, \n\u001b[0;32m    220\u001b[0m                                                                   training=False, monitor=False),\n\u001b[1;32m--> 221\u001b[1;33m                                   epochs=nb_epoch, verbose=1, callbacks=callbacks_list) \n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1064\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    777\u001b[0m     \u001b[1;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m     \u001b[1;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m     \u001b[0mpeek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    834\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    837\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-76-e73c23d6942d>\u001b[0m in \u001b[0;36mbatch_generator\u001b[1;34m(x, y, batch_size, shape, training, data_dir, monitor, yieldXY, discard_rate)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;31m#print(img_address)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                 \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_steering\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_transformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_address\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_steering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[1;31m#img = cv2.imread(data_dir + img_address,0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-76-e73c23d6942d>\u001b[0m in \u001b[0;36mimage_transformation\u001b[1;34m(img_address, degree, data_dir)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0mimg_address\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleft_right_random_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_address\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_address\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_brightness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhorizontal_flip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-6uw63ony\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "### -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, PReLU, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "import os.path\n",
    "import csv\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from keras import callbacks\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "SEED = 13\n",
    "\n",
    "def horizontal_flip(img, degree):\n",
    "    choice = np.random.choice([0, 1])\n",
    "    if choice == 1:\n",
    "        img, degree = cv2.flip(img, 1), -degree\n",
    "    return (img, degree)\n",
    "\n",
    "def random_brightness(img, degree):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    alpha = np.random.uniform(low = 0.1, high = 1.0, size = None)\n",
    "    v = hsv[:, :, 2]\n",
    "    v = v * alpha\n",
    "    hsv[:, :, 2] = v.astype('uint8')\n",
    "    rgb = cv2.cvtColor(hsv.astype('uint8'), cv2.COLOR_HSV2RGB)\n",
    "    return(rgb, degree)\n",
    "\n",
    "def left_right_random_swap(img_address, degree, degree_corr = 1.0/4):\n",
    "    swap = np.random.choice(['L', 'R', 'C'])\n",
    "    if swap == 'L':\n",
    "        img_address = img_address.replace('center','left')\n",
    "        corrected_label = np.arctan(math.tan(degree) + degree_corr)\n",
    "        return (img_address, corrected_label)\n",
    "    elif swap == 'R':\n",
    "        img_address = img_address.replace('center','right')\n",
    "        corrected_label = np.arctan(math.tan(degree) - degree_corr)\n",
    "        return (img_address, corrected_label)\n",
    "    else:\n",
    "        return (img_address, degree)\n",
    "\n",
    "def discard_zero_steering(degrees, rate):\n",
    "    steering_zero_idx = np.where(degrees == 0)\n",
    "    steering_zero_idx = steering_zero_idx[0]\n",
    "    size_del = int(len(steering_zero_idx) * rate)\n",
    "    return np.random.choice(steering_zero_idx, size = size_del, replace = False)\n",
    "\n",
    "def get_model(shape):\n",
    "    '''\n",
    "    预测方向盘角度: 以图像为输入, 预测方向盘的转动角度\n",
    "    shape: 输入图像的尺寸, 例如(128, 128, 3)\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(8, (5, 5), strides=(1, 1), padding=\"valid\", input_shape=shape))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add( Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(8, (5, 5), strides=(1, 1), padding=\"valid\",))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add( Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(16, (4, 4), strides=(1, 1), padding=\"valid\", ))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add( Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(16, (5, 5), strides=(1, 1), padding=\"valid\",))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add( Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add( Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(50))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add( Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(10))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add( Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def image_transformation(img_address, degree, data_dir):\n",
    "    img_address, degree = left_right_random_swap(img_address, degree)\n",
    "    img = cv2.imread(img_address,0)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    img, degree = random_brightness(img, degree)\n",
    "    img, degree = horizontal_flip(img, degree)\n",
    "    return (img, degree)\n",
    "\n",
    "def batch_generator(x, y, batch_size, shape, training=True, data_dir='D:\\\\.zhangwanru\\\\yanyi\\\\yijike\\\\jiqi\\\\CNN\\\\data\\\\', \n",
    "                    monitor=True, yieldXY=True, discard_rate=0.95):  \n",
    "    if training:\n",
    "        y_bag = []\n",
    "        x, y = shuffle(x, y)\n",
    "        new_x = x\n",
    "        new_y = y\n",
    "    else:\n",
    "        new_x = x\n",
    "        new_y = y\n",
    "    \n",
    "    offset = 0\n",
    "    while True: \n",
    "        X = np.empty((batch_size, *shape))\n",
    "        Y = np.empty((batch_size, 1))\n",
    "\n",
    "        for example in range(batch_size):\n",
    "            img_address, img_steering = new_x[example + offset], new_y[example + offset]\n",
    "            #print(img_address)\n",
    "            if training:\n",
    "                img, img_steering = image_transformation(img_address, img_steering, data_dir)\n",
    "            else:\n",
    "                #img = cv2.imread(data_dir + img_address,0)\n",
    "                img = cv2.imread(img_address,0)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            X[example,:,:,:] = cv2.resize(img[80:140, 0:320], (shape[0], shape[1]) ) / 255 - 0.5\n",
    "            \n",
    "            Y[example] = img_steering\n",
    "            if training:\n",
    "                y_bag.append(img_steering)\n",
    "            \n",
    "            if (example + 1) + offset > len(new_y) - 1:\n",
    "                x, y = shuffle(x, y)\n",
    "                rand_zero_idx = discard_zero_steering(y, rate = discard_rate)\n",
    "                new_x = x\n",
    "                new_y = y\n",
    "                new_x = np.delete(new_x, rand_zero_idx, axis=0)\n",
    "                new_y = np.delete(new_y, rand_zero_idx, axis=0)                \n",
    "                offset = 0\n",
    "        if yieldXY:\n",
    "            yield (X, Y)\n",
    "        else:\n",
    "            yield X\n",
    "\n",
    "        offset = offset + batch_size\n",
    "        if training:\n",
    "            np.save('y_bag.npy', np.array(y_bag) )\n",
    "            np.save('Xbatch_sample.npy', X )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    data_path = 'D:\\\\.zhangwanru\\\\yanyi\\\\yijike\\\\jiqi\\\\CNN\\\\data\\\\'\n",
    "    with open(data_path+'driving_log.csv', 'r',encoding='utf-8') as csvfile:\n",
    "        file_reader = csv.reader(csvfile, delimiter=',')\n",
    "        log = []\n",
    "        for row in file_reader:\n",
    "            log.append(row)\n",
    "\n",
    "    log = np.array( log )\n",
    "   \n",
    "    # 判断图像文件数量是否等于csv日志文件中记录的数量\n",
    "    ls_imgs = glob.glob(data_path+'IMG\\\\*.jpg')\n",
    "    assert len(ls_imgs) == len(log)*3, 'number of images does not match'\n",
    "\n",
    "    # 使用20%的数据作为测试数据\n",
    "    validation_ratio = 0.2\n",
    "    shape = (128, 128, 3)\n",
    "    batch_size = 8\n",
    "    nb_epoch = 400\n",
    "\n",
    "    x_ = log[:, 0] \n",
    "    y_ = log[:, 3].astype(float)\n",
    "    x_, y_ = shuffle(x_, y_)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(x_, y_, test_size=validation_ratio, random_state=SEED)\n",
    "\n",
    "    print('batch size: {}'.format(batch_size))\n",
    "    print('Train set size: {} | Validation set size: {}'.format(len(X_train), len(X_val)))\n",
    "        \n",
    "    samples_per_epoch = batch_size \n",
    "    # 使得validation数据量大小为batch_size的整数陪\n",
    "    nb_val_samples = len(y_val) - len(y_val)%batch_size\n",
    "    model = get_model(shape)\n",
    "    print(model.summary())\n",
    "    \n",
    "\n",
    "    # 根据validation loss保存最优模型\n",
    "    save_best = callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', verbose=1, \n",
    "                                         save_best_only=True, mode='min')\n",
    "\n",
    "    # 如果训练持续没有validation loss的提升, 提前结束训练，可通过更改patience参数确定终止条件。                              \n",
    "    early_stop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=400, \n",
    "                                         verbose=0, mode='auto')\n",
    "    callbacks_list = [early_stop, save_best, PlotLossesKeras()]\n",
    "\n",
    "    history = model.fit_generator(batch_generator(X_train, y_train, batch_size, shape, training=True),\n",
    "                                  steps_per_epoch = samples_per_epoch,\n",
    "                                  validation_steps = nb_val_samples // batch_size,\n",
    "                                  validation_data = batch_generator(X_val, y_val, batch_size, shape, \n",
    "                                                                  training=False, monitor=False),\n",
    "                                  epochs=nb_epoch, verbose=1, callbacks=callbacks_list) \n",
    "\n",
    "\n",
    "    with open('./trainHistoryDict.p', 'wb') as file_pi:\n",
    "         pickle.dump(history.history, file_pi)\n",
    "\n",
    "    pyplot.plot(history.history['loss'])\n",
    "    pyplot.plot(history.history['val_loss'])\n",
    "    pyplot.title('model train vs validation loss')\n",
    "    pyplot.ylabel('loss')\n",
    "    pyplot.xlabel('epoch')\n",
    "    pyplot.legend(['train', 'validation'], loc='upper right')\n",
    "    pyplot.savefig('train_val_loss.jpg')\n",
    "\n",
    "    # 保存模型\n",
    "    with open('model.json', 'w') as f:\n",
    "            f.write( model.to_json() )\n",
    "    model.save('model.h5')\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x, y, batch_size, shape, training=True, data_dir='D:\\\\.zhangwanru\\\\yanyi\\\\yijike\\\\jiqi\\\\CNN\\\\data\\\\', \n",
    "                    monitor=True, yieldXY=True, discard_rate=0.95):  \n",
    "    if training:\n",
    "        y_bag = []\n",
    "        x, y = shuffle(x, y)\n",
    "        new_x = x\n",
    "        new_y = y\n",
    "    else:\n",
    "        new_x = x\n",
    "        new_y = y\n",
    "    \n",
    "    offset = 0\n",
    "    while True: \n",
    "        X = np.empty((batch_size, *shape))\n",
    "        Y = np.empty((batch_size, 1))\n",
    "\n",
    "        for example in range(batch_size):\n",
    "            img_address, img_steering = new_x[example + offset], new_y[example + offset]\n",
    "            #print(img_address)\n",
    "            if training:\n",
    "                img, img_steering = image_transformation(img_address, img_steering, data_dir)\n",
    "            else:\n",
    "                img = cv2.imread(data_dir + img_address,0)\n",
    "                #img = cv2.imread(img_address)\n",
    "                #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            X[example,:,:,:] = cv2.resize(img[80:140, 0:320], (shape[0], shape[1]) ) / 255 - 0.5\n",
    "            \n",
    "            Y[example] = img_steering\n",
    "            if training:\n",
    "                y_bag.append(img_steering)\n",
    "            \n",
    "            if (example + 1) + offset > len(new_y) - 1:\n",
    "                x, y = shuffle(x, y)\n",
    "                rand_zero_idx = discard_zero_steering(y, rate = discard_rate)\n",
    "                new_x = x\n",
    "                new_y = y\n",
    "                new_x = np.delete(new_x, rand_zero_idx, axis=0)\n",
    "                new_y = np.delete(new_y, rand_zero_idx, axis=0)                \n",
    "                offset = 0\n",
    "        if yieldXY:\n",
    "            yield (X, Y)\n",
    "        else:\n",
    "            yield X\n",
    "\n",
    "        offset = offset + batch_size\n",
    "        if training:\n",
    "            np.save('y_bag.npy', np.array(y_bag) )\n",
    "            np.save('Xbatch_sample.npy', X )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-6uw63ony\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-750d82d10b97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                   validation_data = batch_generator(X_val, y_val, batch_size, shape, \n\u001b[0;32m      5\u001b[0m                                                                   training=False, monitor=False),\n\u001b[1;32m----> 6\u001b[1;33m                                   epochs=nb_epoch, verbose=1, callbacks=callbacks_list) \n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1064\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    777\u001b[0m     \u001b[1;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m     \u001b[1;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m     \u001b[0mpeek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    834\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m     \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    837\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-065d8a81f4a5>\u001b[0m in \u001b[0;36mbatch_generator\u001b[1;34m(x, y, batch_size, shape, training, data_dir, monitor, yieldXY, discard_rate)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;31m#print(img_address)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                 \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_steering\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_transformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_address\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_steering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mimg_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-065d8a81f4a5>\u001b[0m in \u001b[0;36mimage_transformation\u001b[1;34m(img_address, degree, data_dir)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0mimg_address\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleft_right_random_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_address\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_brightness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhorizontal_flip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-6uw63ony\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(batch_generator(X_train, y_train, batch_size, shape, training=True),\n",
    "                                  steps_per_epoch = samples_per_epoch,\n",
    "                                  validation_steps = nb_val_samples // batch_size,\n",
    "                                  validation_data = batch_generator(X_val, y_val, batch_size, shape, \n",
    "                                                                  training=False, monitor=False),\n",
    "                                  epochs=nb_epoch, verbose=1, callbacks=callbacks_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object batch_generator at 0x0000019928FBE468>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_generator(X_train, y_train, batch_size, shape, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object batch_generator at 0x0000019928FBE258>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_generator(X_val, y_val, batch_size, shape,training=False, monitor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
